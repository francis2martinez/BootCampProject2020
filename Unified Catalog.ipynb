{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a Unified Catalog that contains the following columns:\n",
    "    1. AUTHOR\n",
    "    2. Vangogh\n",
    "    3. URL\n",
    "    4. Image\n",
    "    5. Mean_Strokes\n",
    "    6. Stroke_Patch (Now this includes the path with min number) (Normalized 100x100)\n",
    "    7. Normalized_Image_B (100x100)\n",
    "    8. Normalized_Image_G\n",
    "    9. Normalized_Image_R\n",
    "    10. No_Faces\n",
    "    11. No_Eyes\n",
    "    12. Patch_Face (100x100 grayscale not normalized)\n",
    "    13. GNHist\n",
    "    14. OGNHist\n",
    "    15. Hist_Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to later use the following methods:\n",
    "    1. Logistic regresion for the Mean_Strokes\n",
    "    2. MLP for the Stroke_Patch\n",
    "    3. MLP for the 3 color channels together\n",
    "Finally over those, wi'll make an Ensemble learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the packages\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "from scipy import ndimage\n",
    "from ripser import ripser, lower_star_img\n",
    "from persim import plot_diagrams \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib.patches import Rectangle\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General purpose functions\n",
    "def get_image(url):\n",
    "    resp = urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    return image\n",
    "\n",
    "def plot_image(image):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    b,g,r = cv2.split(image)       # get b,g,r\n",
    "    rgb_img = cv2.merge([r,g,b])     # switch it to rgb\n",
    "\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.xticks([]); plt.yticks([])   # to hide tick values on X and Y axis\n",
    "\n",
    "def preprocess_image_gray(image):\n",
    "    image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = ndimage.uniform_filter(image.astype(np.float64), size=5)\n",
    "    image += 0.01 * np.random.randn(*image.shape)  \n",
    "    return image\n",
    "\n",
    "def normalize_image(image,st_scaler=StandardScaler(),minmax_scaler=MinMaxScaler()):\n",
    "    img=image.copy()\n",
    "    img=cv2.resize(img,(100,100))\n",
    "    img_normal=minmax_scaler.fit_transform(st_scaler.fit_transform(img.reshape(-1, 1)))\n",
    "    img_normal=(img_normal*200).astype(np.uint8)\n",
    "    return img_normal.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for the Strokes Parts\n",
    "def get_patches(img,patch_size=(200,200)):\n",
    "    image=img.copy()\n",
    "    N=image.shape[0]; M=image.shape[1]\n",
    "    x_size=y_size=int(min(N,M)/5)\n",
    "    Patches=[image[y:(y+y_size),x:(x+x_size)].copy() for x in [0,M-x_size] for y in [0,N-y_size]]\n",
    "\n",
    "    #Choose one random in the middle\n",
    "    x=np.random.randint(x_size,M-2*x_size)\n",
    "    y=np.random.randint(y_size,N-2*y_size)\n",
    "\n",
    "    Patches.append(image[y:(y+y_size),x:(x+x_size)].copy())\n",
    "    image[y:(y+y_size),x:(x+x_size)]=0\n",
    "    \n",
    "    #Resize the Patches\n",
    "    Patches=[cv2.resize(img,patch_size) for img in Patches]\n",
    "        \n",
    "    return Patches\n",
    "\n",
    "def get_max_strokes_patch(url,thresh=10):\n",
    "    #Read Image\n",
    "    image=get_image(url)\n",
    "    \n",
    "    #PreProcess\n",
    "    image=preprocess_image_gray(image)\n",
    "           \n",
    "    #Get Patches\n",
    "    Patches=get_patches(image)\n",
    "       \n",
    "    #Compute and plot the Persistance Diagrams\n",
    "    Dgms=[]\n",
    "    No_Strokes=[]\n",
    "    for i in range(5):\n",
    "        patch=Patches[i]\n",
    "        \n",
    "        #Find the lower_star persistence using ripser\n",
    "        #Cite Tom Needham\n",
    "        dgm = lower_star_img(-patch)\n",
    "        Dgms.append(dgm)\n",
    "        \n",
    "        #Get the indices of the persistence classes with life > thresh\n",
    "        idxs = np.arange(dgm.shape[0])\n",
    "        idxs = idxs[np.abs(dgm[:, 1] - dgm[:, 0]) > thresh]\n",
    "        \n",
    "      \n",
    "        #Translate back the persistence classes into points of the original image.\n",
    "        bidxs=[]\n",
    "        for idx in idxs:\n",
    "            bidx = np.argmin(np.abs(patch + dgm[idx, 0]))\n",
    "            bidxs.append(bidx)\n",
    "        bidxs=list(np.unique(bidxs))\n",
    "        \n",
    "        No_Strokes.append(len(bidxs))\n",
    "        \n",
    "    #Return the min, max and mean no. of Brush Strokes\n",
    "    return np.mean(No_Strokes), normalize_image(Patches[np.argmax(No_Strokes)])\n",
    "\n",
    "def add_no_strokes_and_patch(Catalog):\n",
    "    Catalog['Stroke_Patch']=0\n",
    "    Catalog.Stroke_Patch=Catalog.Stroke_Patch.astype('object')\n",
    "    Catalog['Mean_Strokes']=0\n",
    "    N=Catalog.shape[0]\n",
    "    All_Strokes=[]\n",
    "    All_Patches=[]\n",
    "    for ind in Catalog.index:\n",
    "        url=Catalog.URL[ind]\n",
    "        mean_strokes,stroke_patch=get_max_strokes_patch(url,10)\n",
    "        All_Strokes.append(mean_strokes)\n",
    "        All_Patches.append(stroke_patch)\n",
    "        \n",
    "    Catalog['Stroke_Patch']=All_Patches\n",
    "    Catalog['Mean_Strokes']=All_Strokes\n",
    "    \n",
    "    return Catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Normalize each color channel\n",
    "def add_normalized_colors(Catalog):\n",
    "    Catalog['Normalized_Image_B']=0\n",
    "    Catalog.Normalized_Image_B=Catalog.Normalized_Image_B.astype('object')\n",
    "    \n",
    "    Catalog['Normalized_Image_G']=0\n",
    "    Catalog.Normalized_Image_G=Catalog.Normalized_Image_G.astype('object')\n",
    "    \n",
    "    Catalog['Normalized_Image_R']=0\n",
    "    Catalog.Normalized_Image_R=Catalog.Normalized_Image_R.astype('object')\n",
    "    \n",
    "    N=Catalog.shape[0]\n",
    "    All_B=[]; All_G=[]; All_R=[]\n",
    "    st_scaler=StandardScaler();minmax_scaler=MinMaxScaler()\n",
    "    for i in range(N):\n",
    "        image=Catalog.Image[i]\n",
    "        All_B.append(normalize_image(image[:,:,0],st_scaler,minmax_scaler))\n",
    "        All_G.append(normalize_image(image[:,:,1],st_scaler,minmax_scaler))\n",
    "        All_R.append(normalize_image(image[:,:,2],st_scaler,minmax_scaler))\n",
    "    Catalog['Normalized_Image_B']=All_B\n",
    "    Catalog['Normalized_Image_G']=All_G\n",
    "    Catalog['Normalized_Image_R']=All_R\n",
    "    \n",
    "    return Catalog   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to get Histograms\n",
    "def gray_histogram(image,preprocess=True):\n",
    "    #PreProcess\n",
    "    if preprocess:\n",
    "        image=preprocess_image_gray(image)\n",
    "    image=MinMaxScaler().fit_transform(StandardScaler().fit_transform(image.reshape(-1, 1)))\n",
    "    image=np.multiply(image,255)\n",
    "    \n",
    "    #Histogram\n",
    "    hist = cv2.calcHist([np.float32(image)], [0], None, [256], [0, 256])\n",
    "    hist = np.divide(hist,max(hist))\n",
    "    return hist\n",
    "\n",
    "def ordered_histogram(hist):\n",
    "    order_hist=np.sort(hist.reshape(1,-1))\n",
    "    order_hist=np.divide(order_hist.reshape(-1,1),order_hist[0,255])\n",
    "    return order_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to append histogram to an imported Catalog\n",
    "def append_histograms(Catalog):\n",
    "    N=Catalog.shape[0]\n",
    "    histogram=np.zeros((N,256))\n",
    "    Catalog['GNHist']=''\n",
    "    o_histogram=np.zeros((N,256))\n",
    "    Catalog['OGNHist']=''\n",
    "    for ind in Catalog.index:\n",
    "        image=Catalog.Image[ind]\n",
    "        hist=gray_histogram(image).reshape(1,-1)\n",
    "        histogram[ind]=hist\n",
    "        o_histogram[ind]=ordered_histogram(hist).reshape(1,-1)\n",
    "    Catalog.GNHist=histogram.tolist()\n",
    "    Catalog.OGNHist=o_histogram.tolist()\n",
    "    return Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to extract faces\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "face2_cascade=cv2.CascadeClassifier('haarcascade_profileface.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "def extract_faces_eyes(image):\n",
    "    gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray,1.3,3)\n",
    "    faces2=face2_cascade.detectMultiScale(gray,1.3,4)\n",
    "    eyes=eye_cascade.detectMultiScale(gray,1.3,4)\n",
    "    \n",
    "    faces=list(faces)+list(faces2)   \n",
    "    \n",
    "    if len(faces)>0:  \n",
    "        sizes_faces=[f[2]*f[3] for f in faces]\n",
    "        f=faces[np.argmax(sizes_faces)]\n",
    "        Face_Patch=gray[f[1]:(f[1]+f[3]),f[0]:(f[0]+f[2])]\n",
    "        Face_Patch=cv2.resize(Face_Patch,(100,100))\n",
    "        Hist=gray_histogram(Face_Patch,False)\n",
    "        Face_Patch=Face_Patch.ravel()       \n",
    "    else:\n",
    "        Face_Patch=None\n",
    "        Hist=None\n",
    "    \n",
    "    return len(faces),len(eyes), Face_Patch, Hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Faces_Eyes_Catalog(Catalog):\n",
    "    Catalog['Face_Patch']=0\n",
    "    Catalog.Face_Patch=Catalog.Face_Patch.astype('object')\n",
    "    Catalog['No_Faces']=0\n",
    "    Catalog['No_Eyes']=0\n",
    "    Catalog['Hist_Face']=0\n",
    "    Catalog.Hist_Face=Catalog.Hist_Face.astype('object')\n",
    "    N=Catalog.shape[0]\n",
    "    print('Need:', N)\n",
    "    \n",
    "    All_No_Faces=[]\n",
    "    All_No_Eyes=[]\n",
    "    All_Patches=[]\n",
    "    All_Hists=[]\n",
    "    for ind in Catalog.index:\n",
    "        url=Catalog.URL[ind]\n",
    "        no_faces,no_eyes,face_patch,hist=extract_faces_eyes(get_image(url))\n",
    "        All_No_Faces.append(no_faces)\n",
    "        All_No_Eyes.append(no_eyes)\n",
    "        All_Patches.append(face_patch)\n",
    "        All_Hists.append(hist)\n",
    "        \n",
    "    Catalog.No_Faces=All_No_Faces\n",
    "    Catalog.No_Eyes=All_No_Eyes\n",
    "    Catalog.Face_Patch=All_Patches\n",
    "    Catalog.Hist_Face=All_Hists\n",
    "    \n",
    "    return Catalog    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Patches and Strokes\n",
    "#Catalog=pd.read_pickle('Catalog_train.pkl')\n",
    "#Catalog=add_no_strokes_and_patch(Catalog)\n",
    "#pd.to_pickle(Catalog,'Catalogs/Catalog_train_w_strokes.pkl',protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize channels and save\n",
    "#Catalog=pd.read_pickle('Catalogs/Catalog_train_w_strokes.pkl')\n",
    "#Catalog=add_normalized_colors(Catalog)\n",
    "\n",
    "#del Catalog['URL_small']\n",
    "#pd.to_pickle(Catalog,'Catalogs/Catalog_train_w_strokes_and_channels.pkl',protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need: 1060\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "to_pickle() missing 1 required positional argument: 'filepath_or_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4de0cc0baf69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mCatalog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Catalogs/Catalog_train_w_strokes_and_channels.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mCatalog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_Faces_Eyes_Catalog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCatalog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Catalogs/Catalog_train_w_strokes_and_channels_and_faces.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: to_pickle() missing 1 required positional argument: 'filepath_or_buffer'"
     ]
    }
   ],
   "source": [
    "#Count Faces\n",
    "Catalog=pd.read_pickle('Catalogs/Catalog_train_w_strokes_and_channels.pkl')\n",
    "Catalog=add_Faces_Eyes_Catalog(Catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(Catalog,'Catalogs/Catalog_train_w_strokes_and_channels_and_faces.pkl',protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Histograms\n",
    "Catalog=pd.read_pickle('Catalogs/Catalog_train_w_strokes_and_channels_and_faces.pkl')\n",
    "Catalog=append_histograms(Catalog)\n",
    "pd.to_pickle(Catalog,'Catalogs/Catalog_train_w_strokes_and_channels_and_faces_and_hists.pkl',protocol=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Patches and Strokes for the test\n",
    "Catalog=pd.read_pickle('Catalog_test.pkl')\n",
    "Catalog=add_no_strokes_and_patch(Catalog)\n",
    "pd.to_pickle(Catalog,'Catalogs/Catalog_test_w_strokes.pkl',protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize channels and save for the test\n",
    "Catalog=pd.read_pickle('Catalogs/Catalog_test_w_strokes.pkl')\n",
    "Catalog=add_normalized_colors(Catalog)\n",
    "\n",
    "del Catalog['URL_small']\n",
    "pd.to_pickle(Catalog,'Catalogs/Catalog_test_w_strokes_and_channels.pkl',protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need: 265\n"
     ]
    }
   ],
   "source": [
    "#Count Faces\n",
    "Catalog=pd.read_pickle('Catalogs/Catalog_test_w_strokes_and_channels.pkl')\n",
    "Catalog=add_Faces_Eyes_Catalog(Catalog)\n",
    "pd.to_pickle(Catalog,'Catalogs/Catalog_test_w_strokes_and_channels_and_faces.pkl',protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Histograms\n",
    "Catalog=pd.read_pickle('Catalogs/Catalog_test_w_strokes_and_channels_and_faces.pkl')\n",
    "Catalog=append_histograms(Catalog)\n",
    "pd.to_pickle(Catalog,'Catalogs/Catalog_test_w_strokes_and_channels_and_faces_and_hists.pkl',protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
